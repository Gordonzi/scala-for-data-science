{
  "metadata" : {
    "name" : "WhyScala",
    "user_save_timestamp" : "1969-12-31T18:00:00.000Z",
    "auto_save_timestamp" : "1969-12-31T18:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "FFD1A3B3C876454DA66D87714D2AE706"
    },
    "cell_type" : "markdown",
    "source" : "# Scala: the Unpredicted Lingua Franca for Data Science"
  }, {
    "metadata" : {
      "id" : "A032770187B947768ADE83D1C76C6D8B"
    },
    "cell_type" : "markdown",
    "source" : " **Andy Petrella**<br/>[noootsab@data-fellas.guru](mailto:noootsab@data-fellas.guru)<br/>\n **Dean Wampler**<br/>[dean.wampler@lightbend.com](mailto:dean.wampler@lightbend.com)\n\nScala Days NYC, May 5th, 2016\n\nThese notebooks available at [github.com/data-fellas/scala-for-data-science](https://github.com/data-fellas/scala-for-data-science)."
  }, {
    "metadata" : {
      "id" : "89E60F5356A24745AD7FFC3ED747D7E4"
    },
    "cell_type" : "markdown",
    "source" : "## Why Scala for Data Science with Spark?"
  }, {
    "metadata" : {
      "id" : "3B4411F9D439469AB4992A6E8D178757"
    },
    "cell_type" : "markdown",
    "source" : "While Python and R are traditional languages of choice for Data Science, Spark also supports Scala (the language in which it's written) and Java.\n\nHowever, using one language for all work has advantages like simplifying the software development process, such as build and deployment tools, coding conventions, etc.\n\nSo, what are the advantages, as well as disadvantages of Scala?"
  }, {
    "metadata" : {
      "id" : "058C9660033E42EAB7089F7B95B9441A"
    },
    "cell_type" : "markdown",
    "source" : "## 1. Functional Programming Plus Objects\n\nScala is a _multi-paradigm_ language. Code can look a lot like traditional Java code using _Object-Oriented Programming_ (OOP), but it also embraces _Function Programming_ (FP), which emphasizes the virtues of:\n1. **Immutable values:** Mutability is a common source of bugs.\n1. **Functions with no _side effects_:** All the information they need is passed in and all the \"work\" is returned. No external state is modified.\n1. **Referential transparency:** You can replace a function call with a cached value that was returned from a previous invocation with the same arguments. (This is a benefit enabled by functions without side effects.)\n1. **Higher-order functions:** Functions that take other functions as arguments are return functions as results.\n1. **Structure separated from operations:** A core set of collections meets most needs. An operation applicable to one data structure is applicable to all."
  }, {
    "metadata" : {
      "id" : "84C5C921657D4A0B821CD106634135A4"
    },
    "cell_type" : "markdown",
    "source" : "However, objects are still useful as an _encapsulation_ mechanism. This is valuable for projects with large teams and code bases. \nScala also implements some _functional_ features using _object-oriented inheritance_ (e.g., \"abstract data types\" and \"type classes\", for you experts...).\n\nThere are a few differences with Java's vs. Scala's approaches to OOP and FP that are worth mentioning:"
  }, {
    "metadata" : {
      "id" : "584F4A92CB454F0586F5EBC2F4ACD135"
    },
    "cell_type" : "markdown",
    "source" : "## 1a. Traits vs. Interfaces\nScala's object model adds a _trait_ feature, which is a more powerful concept than Java 8 interfaces. Before Java 8, there was no [mixin composition](https://en.wikipedia.org/wiki/Mixin) capability in Java, where composition is generally [preferred over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance). \n\nImagine that you want to define reusable logging code and mix it into other classes declaratively. Before Java 8, you could define the abstraction for logging in an interface, but you had to use some ad hoc mechanism to implement it (like implementing all methods to delegate to a helper object). Java 8 added the ability to provide default method definitions, as well as declarations in interfaces. This makes mixin composition easier, but you still can't add fields (for state), so the capability is limited. \n\nScala traits fully support mixin composition by supporting both field and method definitions with flexibility rules for overriding behavior, once the traits are mixed into classes."
  }, {
    "metadata" : {
      "id" : "DC6C322EAB2244B9844CB0FDABCE6BA3"
    },
    "cell_type" : "markdown",
    "source" : "## 1b. Java Streams\nWhen you use the Java 8 collections, you can convert the the tradition collections to a \"stream\", which is lazy and gives you more functional operations. However, sometimes, the conversions back and forth can be tedious, e.g., converting to a stream for functional processing, then converting pass them to older APIs, etc. Scala collections are more consistently functional."
  }, {
    "metadata" : {
      "id" : "F76E6ACFA660449C8B4CEDE9289C0DD9"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Supports mixed FP-OOP programming, too, but isn't as \"rigorous\". \n* **R:** As a Statistics language, R is more functional than object-oriented.\n* **Java:** An object-oriented language, but with recently introduced functional constructs, _lambdas_ (anonymous functions) and collection operations that follow a more _functional_ style, rather than _imperative_ (i.e., where mutating the collection is embraced)."
  }, {
    "metadata" : {
      "id" : "E706C1FC94B34C5E86C25040BDF9263C"
    },
    "cell_type" : "markdown",
    "source" : "Let's examine how concise we can operate on a collection of values in Scala and Spark."
  }, {
    "metadata" : {
      "id" : "358539665C7D40479DC6FB0910C08FBE"
    },
    "cell_type" : "markdown",
    "source" : "First, a helper function: is an integer a prime? (NaÃ¯ve algorithm from [Wikipedia](https://en.wikipedia.org/wiki/Primality_test).)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EB02D7EAB7F8406A849C22CE2528B443"
    },
    "cell_type" : "code",
    "source" : "def isPrime(n: Int): Boolean = {\n  def test(i: Int, n2: Int): Boolean = {\n    if (i*i > n2) true\n    else if (n2 % i == 0 || n2 % (i + 2) == 0) false\n    else test(i+6, n2)\n  }\n  if (n <= 1) false\n  else if (n <= 3) true\n  else if (n % 2 == 0 || n % 3 == 0) false\n  else test(5, n)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "isPrime: (n: Int)Boolean\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : {
      "id" : "81C67BC34835425582115F8FD36BCEC3"
    },
    "cell_type" : "markdown",
    "source" : "Note that no values are mutated here (#1 \"virtue\" listed above) and `isPrime` has no side effects (#2), which means we could cache previous invocations for a given `n` for better performance if we called this a lot (#3)!"
  }, {
    "metadata" : {
      "id" : "1B606E0028B245438CA7BE870A1AF082"
    },
    "cell_type" : "markdown",
    "source" : "### Scala Collections Example\nLet's compare a Scala collections calculation vs. the same thing in Spark; how many prime numbers are there between 1 and 100, inclusive?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1746844178-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "5E9DCA78D8DB4E5E8362EB9138F380F5"
    },
    "cell_type" : "code",
    "source" : "(1 until 100).                 // Range of integers from 1 to 100, inclusive.\n  map(i => (i, isPrime(i))).   // `map` is a higher-order method; we pass it a function (#4)\n  groupBy(tuple => tuple._2).  // ... and so is `groupBy`, etc.\n  map(tuple => (tuple._1, tuple._2.size))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res2: scala.collection.immutable.Map[Boolean,Int] = Map(false -> 74, true -> 25)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon60d0d638a73ac297c76bc4d173ab0618&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;1746844178&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <div>\n          <ul class=\"nav nav-tabs\" id=\"ul1746844178\"><li>\n                <a href=\"#tab1746844178-0\"><i class=\"fa fa-table\"/></a>\n              </li><li>\n                <a href=\"#tab1746844178-1\"><i class=\"fa fa-bar-chart\"/></a>\n              </li><li>\n                <a href=\"#tab1746844178-2\"><i class=\"fa fa-pie-chart\"/></a>\n              </li><li>\n                <a href=\"#tab1746844178-3\"><i class=\"fa fa-cubes\"/></a>\n              </li></ul>\n\n          <div class=\"tab-content\" id=\"tab1746844178\"><div class=\"tab-pane\" id=\"tab1746844178-0\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anond37859b920ecaa631580241f8b4f971f&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;1275054369&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon9dc96d676888e5a60dbcd7ab6beb5e2a&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anond457ac177834981fd32209f52b33e419&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div><div class=\"tab-pane\" id=\"tab1746844178-1\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon550ee93caf122d105d117d9fe9b3a079&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;165712073&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/barChart'], \n      function(playground, _magicbarChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicbarChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon92c70d8bf8bec2654cf70552f36ba47f&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anond625a75cd2add8d6776206e950b59084&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div><div class=\"tab-pane\" id=\"tab1746844178-2\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonfd5d5a30ad47dd186e68602e65232776&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;1452593136&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pieChart'], \n      function(playground, _magicpieChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpieChart,\n    \"o\": {\"series\":\"_1\",\"p\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anondd2ea07c45b6da6d19e39a87e5502757&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon84bae9789f9045cca026c4e1f61b963e&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div><div class=\"tab-pane\" id=\"tab1746844178-3\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon8c2e9a879e34b1a6b32c4b97a2852d8b&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;248435790&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon82c2f09e5b9209dbf893d0e424e80cb9&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonafc3766f47aca45beef8a8410a9d1e38&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div></div>\n        </div>\n      </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : {
      "id" : "5638A2D79A1E4073AE3DBA7342623D09"
    },
    "cell_type" : "markdown",
    "source" : "### Spark Example\n\nNote how similar the following code is to the previous example. After constructing the data set, the \"core\" three lines are _identical_, even though they are operating on completely different underlying collections (#5 above). \n\nHowever, because Spark collections are \"lazy\" by default (i.e., not evaluated until we ask for results), we explicitly print the results so Spark evaluates them!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab469772104-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "5B884B3F73BF4D0699067F8850A499AD"
    },
    "cell_type" : "code",
    "source" : "val rddPrimes = sparkContext.parallelize(1 until 100).\n  map(i => (i, isPrime(i))).\n  groupBy(tuple => tuple._2).\n  map(tuple => (tuple._1, tuple._2.size))\nrddPrimes.collect",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rddPrimes: org.apache.spark.rdd.RDD[(Boolean, Int)] = MapPartitionsRDD[4] at map at <console>:58\nres4: Array[(Boolean, Int)] = Array((false,74), (true,25))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon9ca18525559af4dc244c8546ac3db59b&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;469772104&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <div>\n          <ul class=\"nav nav-tabs\" id=\"ul469772104\"><li>\n                <a href=\"#tab469772104-0\"><i class=\"fa fa-table\"/></a>\n              </li><li>\n                <a href=\"#tab469772104-1\"><i class=\"fa fa-bar-chart\"/></a>\n              </li><li>\n                <a href=\"#tab469772104-2\"><i class=\"fa fa-pie-chart\"/></a>\n              </li><li>\n                <a href=\"#tab469772104-3\"><i class=\"fa fa-cubes\"/></a>\n              </li></ul>\n\n          <div class=\"tab-content\" id=\"tab469772104\"><div class=\"tab-pane\" id=\"tab469772104-0\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonc128061e2a95e842d1407f52989ae01f&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;1749908230&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonc13210173bade8702929e404db5356df&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon65a9760e3e906fc93e46475a437b3398&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div><div class=\"tab-pane\" id=\"tab469772104-1\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon0c0ceb3cacc1998b06ac1b4a8632af55&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;1993794941&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/barChart'], \n      function(playground, _magicbarChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicbarChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon51035be7a606f7667371141573199e0e&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon2f8b211cf6a74a740dd18224916eb171&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div><div class=\"tab-pane\" id=\"tab469772104-2\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonfee28d42bfb3780ef5b221b4e9eefff8&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;1327952367&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pieChart'], \n      function(playground, _magicpieChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpieChart,\n    \"o\": {\"series\":\"_1\",\"p\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonb3b463d66a1ef4483fb140223129dd8d&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonf581c2cc0f4adb8287f84b3136ab5b89&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div><div class=\"tab-pane\" id=\"tab469772104-3\">\n              <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon74833061d8538fd952b307f08fc9c611&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:false,&quot;_2&quot;:74},{&quot;_1&quot;:true,&quot;_2&quot;:25}],&quot;genId&quot;:&quot;327417516&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon9db4092f66ba77626f7c28deea3be4af&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon0800400139f666315a0318e926c7aab4&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>\n              </div></div>\n        </div>\n      </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : {
      "id" : "DBAEF93047F04B438C18674E104C0BA0"
    },
    "cell_type" : "markdown",
    "source" : "Spark's RDD API is inspired by the Scala collections API, which is inspired by classic _functional programming_ operations on data collections, i.e., using a series of transformations from one form to the next, without mutating any of the collections. (Spark is very efficient about avoiding the materialization of intermediate outputs.)\n\nOnce you know these operations, it's quick and effective to implement robust, non-trivial transformations."
  }, {
    "metadata" : {
      "id" : "59060F32E3C34640857B9E323B062DB6"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Supports very similar functional programming. In fact, Spark Python code looks very similar to Spark Scala code. \n* **R:** More idiomatic (see below).\n* **Java:** Looks similar when _lambdas_ are used, but missing features (see below) limit concision and flexibility."
  }, {
    "metadata" : {
      "id" : "898023452F2442E38BD08B35E2F3B05A"
    },
    "cell_type" : "markdown",
    "source" : "# 2. Interpreter (REPL)"
  }, {
    "metadata" : {
      "id" : "72A05DBF93454EAB840C4D9E25929017"
    },
    "cell_type" : "markdown",
    "source" : "We've been using the Scala interpreter (a.k.a., the REPL - Read Eval, Print, Loop) already behind the scenes. It makes notebooks like this one possible!"
  }, {
    "metadata" : {
      "id" : "DA296C929D1B43A191C22FAEB538458F"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Also has an interpreter and [iPython/Jupyter](https://ipython.org/) was one of the first, widely-used notebook environments.\n* **R:** Also has an interpreter and notebook/IDE environments.\n* **Java:** Does _not_ have an interpreter and can't be programmed in a notebook environment."
  }, {
    "metadata" : {
      "id" : "32F89F305EFE4D2B9FF61F0B357A0BA7"
    },
    "cell_type" : "markdown",
    "source" : "# 3. Tuple Syntax\nIn data, you work with records of `n` fields (for some value of `n`) all the time. Support for `n`-element _tuples_ is very convenient and Scala has a shorthand syntax for instantiating tuples. We used it twice previously to return two-element tuples in the anonymous functions passed to the `map` methods above:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2C28CB38977246F8AC8F202459D99B43"
    },
    "cell_type" : "code",
    "source" : "sparkContext.parallelize(1 until 100).\n  map(i => (i, isPrime(i))).                // <-- here\n  groupBy(tuple => tuple._2).\n  map(tuple => (tuple._1, tuple._2.size))   // <-- here",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res6: org.apache.spark.rdd.RDD[(Boolean, Int)] = MapPartitionsRDD[9] at map at <console>:60\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[9] at map at &lt;console&gt;:60"
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "id" : "4BA733B4AA944F969B5F1DE83B7D3220"
    },
    "cell_type" : "markdown",
    "source" : "**This is used all the time** in Spark Scala RDD code, where it's common to use key-value pairs."
  }, {
    "metadata" : {
      "id" : "445D28CB9F5347088A038275A7390447"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Also has some support for the same tuple syntax.\n* **R:** Also has tuple types, but a less convenient syntax for instantiating them.\n* **Java:** Does _not_ have tuple types, not even the special case of two-element tuples (pairs), much less a convenient syntax for them. However, Spark defines a [MutablePair](http://spark.apache.org/docs/latest/api/java/org/apache/spark/util/MutablePair.html) type for this purpose."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C4162770FDDA49438DE4E54638979322"
    },
    "cell_type" : "code",
    "source" : "// Using Scala syntax here:\nimport org.apache.spark.util.MutablePair\nval pair = new MutablePair[Int,String](1, \"one\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.util.MutablePair\npair: org.apache.spark.util.MutablePair[Int,String] = (1,one)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : {
      "id" : "787A952B7EDD45329BFE71BDF82617B4"
    },
    "cell_type" : "markdown",
    "source" : "# 4. Pattern Matching\nThis is one of the most powerful features you'll find in most functional languages, Scala included. It has no equivalent in Python, R, or Java.\n\nLet's rewrite our previous primes example:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "48179E0BC6C3404289378596471F1D11"
    },
    "cell_type" : "code",
    "source" : "sparkContext.parallelize(1 until 100).\n  map(i => (i, isPrime(i))).\n  groupBy{ case (_, primality) => primality}.  // Syntax: { case pattern => body }\n  map{ case (primality, values) => (primality, values.size) } . // used here, too\n  foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(true,25)\n(false,74)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : {
      "id" : "8E8F39DFB50F44118B9095470C560E2B"
    },
    "cell_type" : "markdown",
    "source" : "Note the `case` keyword and `=>` separating the pattern from the body to execute if the pattern matches.\n\nIn the first pattern, `(_, primality)`, we didn't need the first tuple element, so we used the \"don't care\" placeholder, `_`. Note also that `{...}` must be used instead of `(...)`. (The extra whitespace after the `{` and before the `}` is not required; it's here for legibility.)\n\nPattern matching is much richer, while more concise than `if ... else ...` constructs in the other languages and we can use it on nearly anything to match what it is and then decompose it into its constituent parts, which are assigned to variables with meaningful names, e.g., `primality`, `values`, etc. "
  }, {
    "metadata" : {
      "id" : "398B4FFC54784BD18F7C2F38EF8F9223"
    },
    "cell_type" : "markdown",
    "source" : "Here's another example, where we deconstruct a nested tuple. We also show that you can use pattern matching for assignment, too!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "722539C191BC4E09B50334373DE61C7E"
    },
    "cell_type" : "code",
    "source" : "val (a, (b1, (b21, b22)), c) = (\"A\", (\"B1\", (\"B21\", \"B22\")), \"C\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "a: String = A\nb1: String = B1\nb21: String = B21\nb22: String = B22\nc: String = C\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : {
      "id" : "288036612A4D4267890DC7DDA156C8CA"
    },
    "cell_type" : "markdown",
    "source" : "# 5. Case Classes \nNow is a good time to introduce a convenient way to declare classes that encapsulate some state that is composed of some values, called _case classes_."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BFE7303C687B473885B74C2784C73B72"
    },
    "cell_type" : "code",
    "source" : "case class Person(firstName: String, lastName: String, age: Int)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class Person\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : {
      "id" : "9901608E6BBA45CCBBAEC1F0EDDD8B99"
    },
    "cell_type" : "markdown",
    "source" : "The `case` keyword tells the compiler to:\n* Make immutable instance fields out of the constructor arguments (the list after the name).\n* Add `equals`, `hashCode`, and `toString` methods (which you can explicitly define yourself, if you want).\n* Add a _companion object_ with the same name, which holds methods for constructing instances and \"destructuring\" instances through patterning matching.\n* etc.\n\nCase classes are useful for implementing records in RDDs.\n\nLet's see case class pattern matching in action:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D3745DD75E024E7C97A653963381E756"
    },
    "cell_type" : "code",
    "source" : "sparkContext.\n  parallelize(Seq(Person(\"Dean\", \"Wampler\", 39), Person(\"Andy\", \"Petrella\", 29))).\n  map { \n    case Person(first, last, age) => (first, last, age)  // Convert Person instances to tuples\n  }.\n  foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(Dean,Wampler,39)\n(Andy,Petrella,29)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : {
      "id" : "8758F66233004E7A9DDA3CF46A6D586D"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Regular expression matching for strings is built in. Pattern matching as shown requires a third-party library with an idiomatic syntax. Nothing like case classes.\n* **R:** Only supports regular expression matching for strings. Nothing like case classes.\n* **Java:** Only supports regular expression matching for strings. Nothing like case classes."
  }, {
    "metadata" : {
      "id" : "DB4A6293E81B4C748AE7C26E2B393EA2"
    },
    "cell_type" : "markdown",
    "source" : "# 6. Type Inference\nMost languages associate a type with values, but they fall into two categories, crudely speaking, those which evaluate the type of expressions and variables at compile time (like Scala and Java) and those which do so at runtime (Python and R). This is call _static typing_ and _dynamic typing_, respectively.\n\nSo, languages with static typing either have to be told the type of every expression or variable, or they can _infer_ types in some or all cases. Scala can infer types most of the time, while Java can do so only in limited cases. Here are some examples for Scala. Note the results shown for each expression:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BB8E2DBE4DB649138913CFA2BE9016D5"
    },
    "cell_type" : "code",
    "source" : "val i = 100       // <- infer that i is an integer\nval j = i*i % 27  // <- since i is an integer, j must be one, too.",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "i: Int = 100\nj: Int = 10\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : {
      "id" : "E5A4DAF4682144A7B07B20E2B334B422"
    },
    "cell_type" : "markdown",
    "source" : "Here's an example with Spark Scala:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "261F068BDF344DA6B96DC784AEB86F19"
    },
    "cell_type" : "code",
    "source" : "sparkContext.parallelize(1 until 100).\n  map(i => (i, isPrime(i))).\n  groupBy{ case(_, primality) => primality }.                  // Syntax: { case pattern => body }\n  map{ case (primality, values) => (primality, values.size) }  // used here, too",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res16: org.apache.spark.rdd.RDD[(Boolean, Int)] = MapPartitionsRDD[21] at map at <console>:63\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[21] at map at &lt;console&gt;:63"
      },
      "output_type" : "execute_result",
      "execution_count" : 11
    } ]
  }, {
    "metadata" : {
      "id" : "B62A3D05041F4E24860434EE37D1AE81"
    },
    "cell_type" : "markdown",
    "source" : "So this long expression (and it is a four-line expression - note the \".\"'s) returns an `RDD[(Boolean, Int)]`. Note that we can also express a tuple _type_ with the `(...)` syntax, just like for tuple _instances_. This type could also be written `RDD[Tuple2[Boolean, Int]]`.\n\nPut another way, we have an `RDD` where the records are key-value pairs of `Booleans` and `Ints`."
  }, {
    "metadata" : {
      "id" : "D2DA2C79231F4685908D8F9A211FD8E2"
    },
    "cell_type" : "markdown",
    "source" : "I really like the extra safety that static typing provides, without the hassle of writing the types for almost everything, compared to Java. Furthermore, when I'm using an API with the Scala interpreter or a notebook like this one, the return value's type is shown, as in the previous example, so I know exactly what \"kinds of things\" I have. That also means I don't have to know _in advance_ what a method will return, in order to explicit add a required type, as in Java."
  }, {
    "metadata" : {
      "id" : "AF7FAC15A7984A878800DC544E1D663F"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Uses dynamic typing, so no types are written explicitly, but you also don't get the feedback type inference provides, as in our `RDD[(Boolean, Int)]` example.\n* **R:** Also dynamically typed.\n* **Java:** Statically typed with explicit types required almost everywhere."
  }, {
    "metadata" : {
      "id" : "37172DAD447A41DC8AB60C19A92B64E5"
    },
    "cell_type" : "markdown",
    "source" : "# 7. Unification of Primitives and Types\nIn Java, there is a clear distinction between primitives, which are nice for performance (you can put them in registers, you can pass them on the stack, you don't heap allocate them), and instances of classes, which give you the expressiveness of OOP, but with the overhead of heap allocation, etc.\n\nScala unifies the syntax, but in most cases, compiles optimal code. So, for example, `Float` acts like any other type, e.g., `String`, with methods you call, but the compiler uses JVM `float` primitives. `Float` and the other primitives are subtypes of `AnyVal` and include `Byte`, `Short`, `Int`, `Long`, `Float`, `Double`, `Char`, `Boolean`, and `Unit`.\n\nAnother benefit is that the uniformity extends to parameterized types, like collections. If you implement your own `Tree[T]` type, `T` can be `Float`, `String`, `MyMassiveClass`, whatever. There's no mental burden of explicitly boxing and unboxing primitives.\n\nHowever, the downside is that your primitives will be boxed when used in a context like this. Scala does have an annotation `@specialized(a,b,c)` that's used to tell the compiler to generate optimal implementations for the primitives listed for `a,b,c`, but it's not a perfect solution.\n\nSee also **Value Classes** below."
  }, {
    "metadata" : {
      "id" : "F9B2730EB3D54267B924086416959197"
    },
    "cell_type" : "markdown",
    "source" : "# 8. Elegant Tools to Create \"Domain Specific Languages\"\nThe Spark [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) API is a good example of DSL that mimics the original Python and R DataFrame APIs for single-node use. \n\nFirst, set up the API..."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "41D0710CC4CB446D869065D358E189CD"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.sql.SQLContext\nval sqlContext = new SQLContext(sparkContext)\nimport sqlContext.implicits._ \nimport org.apache.spark.sql.functions._  // for min, max, etc. column operations",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.SQLContext\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@6b664e54\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B32809D4A5EE4F29846A6CCFD9B19BCE"
    },
    "cell_type" : "code",
    "source" : "// Get the current working directory:\nval root = sys.env(\"NOTEBOOKS_DIR\")  // actually returns root directory of Spark Notebook ",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root: String = /Users/deanwampler/Documents/Conferences/scala-for-data-science/notebooks\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "78D3E11A8F8F45EE86E884812BCE9564"
    },
    "cell_type" : "code",
    "source" : "val airportsDF = sqlContext.read.json(s\"$root/airports.json\")\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "airportsDF: org.apache.spark.sql.DataFrame = [airport: string, city: string, country: string, iata: string, lat: double, long: double, state: string]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 14
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C4943349BC644D468A72E623BE10F17A"
    },
    "cell_type" : "code",
    "source" : "airportsDF.cache\nairportsDF.show",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+--------------------+------------------+-------+----+-----------+------------+-----+\n|             airport|              city|country|iata|        lat|        long|state|\n+--------------------+------------------+-------+----+-----------+------------+-----+\n|            Thigpen |       Bay Springs|    USA| 00M|31.95376472|-89.23450472|   MS|\n|Livingston Municipal|        Livingston|    USA| 00R|30.68586111|-95.01792778|   TX|\n|         Meadow Lake|  Colorado Springs|    USA| 00V|38.94574889|-104.5698933|   CO|\n|        Perry-Warsaw|             Perry|    USA| 01G|42.74134667|-78.05208056|   NY|\n|    Hilliard Airpark|          Hilliard|    USA| 01J| 30.6880125|-81.90594389|   FL|\n|   Tishomingo County|           Belmont|    USA| 01M|34.49166667|-88.20111111|   MS|\n|         Gragg-Wade |           Clanton|    USA| 02A|32.85048667|-86.61145333|   AL|\n|             Capitol|        Brookfield|    USA| 02C|   43.08751|-88.17786917|   WI|\n|   Columbiana County|    East Liverpool|    USA| 02G|40.67331278|-80.64140639|   OH|\n|    Memphis Memorial|           Memphis|    USA| 03D|40.44725889|-92.22696056|   MO|\n|      Calhoun County|         Pittsboro|    USA| 04M|33.93011222|-89.34285194|   MS|\n|    Hawley Municipal|            Hawley|    USA| 04Y|46.88384889|-96.35089861|   MN|\n|Griffith-Merrillv...|          Griffith|    USA| 05C|41.51961917|-87.40109333|   IN|\n|Gatesville - City...|        Gatesville|    USA| 05F|31.42127556|-97.79696778|   TX|\n|              Eureka|            Eureka|    USA| 05U|39.60416667|-116.0050597|   NV|\n|    Moton  Municipal|          Tuskegee|    USA| 06A|32.46047167|-85.68003611|   AL|\n|          Schaumburg|Chicago/Schaumburg|    USA| 06C|41.98934083|-88.10124278|   IL|\n|     Rolla Municipal|             Rolla|    USA| 06D|48.88434111|-99.62087694|   ND|\n|    Eupora Municipal|            Eupora|    USA| 06M|33.53456583|-89.31256917|   MS|\n|            Randall |        Middletown|    USA| 06N|41.43156583|-74.39191722|   NY|\n+--------------------+------------------+-------+----+-----------+------------+-----+\nonly showing top 20 rows\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15
    } ]
  }, {
    "metadata" : {
      "id" : "654B4596BCFE41FD99E64DCFDC9314C7"
    },
    "cell_type" : "markdown",
    "source" : "Okay, now here's the idiomatic DataFrame API:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2DF9D1306DAB41319E03DD8322721545"
    },
    "cell_type" : "code",
    "source" : "val grouped = airportsDF.groupBy($\"state\", $\"country\").count.orderBy($\"count\".desc)\ngrouped.printSchema\ngrouped.show(100)  // 50 states + territories",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- count: long (nullable = false)\n\n+-----+-------+-----+\n|state|country|count|\n+-----+-------+-----+\n|   AK|    USA|  263|\n|   TX|    USA|  209|\n|   CA|    USA|  205|\n|   OK|    USA|  102|\n|   FL|    USA|  100|\n|   OH|    USA|  100|\n|   NY|    USA|   97|\n|   GA|    USA|   97|\n|   MI|    USA|   94|\n|   MN|    USA|   89|\n|   IL|    USA|   88|\n|   WI|    USA|   84|\n|   KS|    USA|   78|\n|   IA|    USA|   78|\n|   MO|    USA|   74|\n|   AR|    USA|   74|\n|   NE|    USA|   73|\n|   AL|    USA|   73|\n|   MS|    USA|   72|\n|   NC|    USA|   72|\n|   MT|    USA|   71|\n|   PA|    USA|   71|\n|   TN|    USA|   70|\n|   IN|    USA|   65|\n|   WA|    USA|   65|\n|   AZ|    USA|   59|\n|   SD|    USA|   57|\n|   OR|    USA|   57|\n|   LA|    USA|   55|\n|   ND|    USA|   52|\n|   SC|    USA|   52|\n|   NM|    USA|   51|\n|   KY|    USA|   50|\n|   CO|    USA|   49|\n|   VA|    USA|   47|\n|   ID|    USA|   37|\n|   NJ|    USA|   35|\n|   UT|    USA|   35|\n|   ME|    USA|   34|\n|   NV|    USA|   32|\n|   WY|    USA|   32|\n|   MA|    USA|   30|\n|   WV|    USA|   24|\n|   MD|    USA|   18|\n|   HI|    USA|   16|\n|   CT|    USA|   15|\n|   NH|    USA|   14|\n|   VT|    USA|   13|\n|   PR|    USA|   11|\n|   RI|    USA|    6|\n|   VI|    USA|    5|\n|   DE|    USA|    5|\n|   CQ|    USA|    4|\n|   AS|    USA|    3|\n|   GU|    USA|    1|\n|   DC|    USA|    1|\n+-----+-------+-----+\n\ngrouped: org.apache.spark.sql.DataFrame = [state: string, country: string, count: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 16
    } ]
  }, {
    "metadata" : {
      "id" : "5B5B48609FAB4F18BA0AD49AE547F028"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Dynamically-typed languages often have features that make idiomatic DSLs easy to define. The Spark DataFrame API is inspired by the [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) API.\n* **R:** Less flexible for idiomatic DSLs, but syntax is designed for Mathematics. The Pandas DataFrame API is inspired by the [R Data Frame](http://www.r-tutor.com/r-introduction/data-frame) API.\n* **Java:** Limited to so-called _fluent_ APIs, similar to our collections and RDD examples above."
  }, {
    "metadata" : {
      "id" : "D098E3AAA94942EDBEC629B643C53F4A"
    },
    "cell_type" : "markdown",
    "source" : "# 9. And a Few Other Things...\nHere a few other useful features. Some are actually quite significant in general programming tasks, but used less in Spark code."
  }, {
    "metadata" : {
      "id" : "563FAC0C5F894273839564136EFD4287"
    },
    "cell_type" : "markdown",
    "source" : "## 9A. Singletons Are a Built-in Feature\nImplement the _Singleton Design Pattern_ without special logic to ensure there's only one instance."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D5EFC6E128DC4BD9840CAE8A94EE1DA7"
    },
    "cell_type" : "code",
    "source" : "object Foo {\n  def main(args: Array[String]):Unit = {\n    args.foreach(arg => println(s\"arg = $arg\"))\n  }\n}\nFoo.main(Array(\"Scala\", \"is\", \"great!\"))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "arg = Scala\narg = is\narg = great!\ndefined object Foo\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 17
    } ]
  }, {
    "metadata" : {
      "id" : "CF2FC2CC60AA4D1982EF940CDDF28A67"
    },
    "cell_type" : "markdown",
    "source" : "## 9B. Named and Default Arguments\nDoes a method have a long argument list? Provide defaults for some of them. Name the arguments when calling the method to document what you're doing."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "63781B531D29467EAA44AC81CF96E7DF"
    },
    "cell_type" : "code",
    "source" : "val airportsRDD = grouped.select($\"count\", $\"state\").rdd.map(row => (row.getLong(0), row.getString(1)))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "airportsRDD: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[57] at map at <console>:69\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 18
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4AB3F2B4D4DD4E70A20D31E4568CE074"
    },
    "cell_type" : "code",
    "source" : "val rdd1 = airportsRDD.sortByKey() // defaults: ascending = true, numPartitions = /* current # of partitions */\nval rdd2 = airportsRDD.sortByKey(ascending = false) // name the ascending argument explicitly\nval rdd3 = airportsRDD.sortByKey(numPartitions = 4) // name the numPartitions argument explicitly\nval rdd4 = airportsRDD.sortByKey(ascending = false, numPartitions = 4) // Okay to do both...",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd1: org.apache.spark.rdd.RDD[(Long, String)] = ShuffledRDD[60] at sortByKey at <console>:71\nrdd2: org.apache.spark.rdd.RDD[(Long, String)] = ShuffledRDD[63] at sortByKey at <console>:72\nrdd3: org.apache.spark.rdd.RDD[(Long, String)] = ShuffledRDD[66] at sortByKey at <console>:73\nrdd4: org.apache.spark.rdd.RDD[(Long, String)] = ShuffledRDD[69] at sortByKey at <console>:74\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 19
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5D959F19FE7E44588FDC45A18368A06C"
    },
    "cell_type" : "code",
    "source" : "Seq(rdd1, rdd2, rdd3, rdd4).foreach { rdd => \n  println(s\"RDD (#partitions = ${rdd.partitions.length}):\")\n  rdd.take(10).foreach(println)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "RDD (#partitions = 41):\n(1,GU)\n(1,DC)\n(3,AS)\n(4,CQ)\n(5,VI)\n(5,DE)\n(6,RI)\n(11,PR)\n(13,VT)\n(14,NH)\nRDD (#partitions = 41):\n(263,AK)\n(209,TX)\n(205,CA)\n(102,OK)\n(100,OH)\n(100,FL)\n(97,NY)\n(97,GA)\n(94,MI)\n(89,MN)\nRDD (#partitions = 4):\n(1,GU)\n(1,DC)\n(3,AS)\n(4,CQ)\n(5,VI)\n(5,DE)\n(6,RI)\n(11,PR)\n(13,VT)\n(14,NH)\nRDD (#partitions = 4):\n(263,AK)\n(209,TX)\n(205,CA)\n(102,OK)\n(100,OH)\n(100,FL)\n(97,NY)\n(97,GA)\n(94,MI)\n(89,MN)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20
    } ]
  }, {
    "metadata" : {
      "id" : "6F439C6371434BD39E75E6753164273A"
    },
    "cell_type" : "markdown",
    "source" : "## 9C. String Interpolation\nYou've seen it already!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6BEA262B490540E79A21B1A1282F54C3"
    },
    "cell_type" : "code",
    "source" : "println(s\"RDD #partitions = ${rdd4.partitions.length}\")\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "RDD #partitions = 4\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 21
    } ]
  }, {
    "metadata" : {
      "id" : "17123565F5964C05A302350950C80E2A"
    },
    "cell_type" : "markdown",
    "source" : "## 9D. No Semicolons\nSemicolons are inferred, making your code just that much more concise. You can use them if you want to write more than one expression on a line:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BF39CCF4418042FCBDE25C3E3CFD7008"
    },
    "cell_type" : "code",
    "source" : "val result = \"foo\" match {\n  case \"foo\" => println(\"Found foo!\"); true\n  case _ => false\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Found foo!\nresult: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 22
    } ]
  }, {
    "metadata" : {
      "id" : "565ED52438AC46ADAC0BC8C82FDC8212"
    },
    "cell_type" : "markdown",
    "source" : "## 9E. Tail Recursion Optimization\n\nRecursion isn't used much in user code for Spark, but for general programming it's a powerful technique. Unfortunately, most OO languages (like Java) do not optimize [tail call recursion](https://en.wikipedia.org/wiki/Tail_call) by converting the recursion into a loop. Without this optimization, use of recursion is risky, because of the risk of stack overflow. Scala's compiler implements this optimization. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "ECAEC625BFBD4B82957F5D81EE524341"
    },
    "cell_type" : "code",
    "source" : "def printSeq[T](seq: Seq[T]): Unit = seq match {\n  case head +: tail => println(head); printSeq(tail)\n  case Nil => // done\n}\nprintSeq(Seq(1,2,3,4))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "1\n2\n3\n4\nprintSeq: [T](seq: Seq[T])Unit\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 23
    } ]
  }, {
    "metadata" : {
      "id" : "FAD283C29E41425E89594C7DBC3122E7"
    },
    "cell_type" : "markdown",
    "source" : "## 9F. Everything Is an Expression\nSome constructs are _statements_ (meaning they return nothing) in some languages, like `if ... then ... else`, `for` loops, etc. Almost everything is an expression in Scala which means you can assign results of the `if` or `for` expression. The alternative in the other languages is that you have to declare a mutable variable, then set its value inside the statement."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EF9F87A5DF6A4589AE09E2F8FD05279E"
    },
    "cell_type" : "code",
    "source" : "val worldRocked = if (true == false) \"yes!\" else \"no\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "worldRocked: String = no\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 24
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab478386907-0\"\n}"
      },
      "id" : "9351539BEF5F42369638265DCD40C0FD"
    },
    "cell_type" : "code",
    "source" : "val primes = for {\n  i <- 0 until 100\n  if isPrime(i)\n} yield i",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "primes: scala.collection.immutable.IndexedSeq[Int] = Vector(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 25
    } ]
  }, {
    "metadata" : {
      "id" : "12E785FDCA514870A0DE2CFA8F739388"
    },
    "cell_type" : "markdown",
    "source" : "## 9G. Implicits\nOne of Scala's most powerful features is the _implicits_ mechanism. It's used (or misused) for several capabilities, but one of the most useful is the ability to \"add\" methods to existing types that don't already have the methods. What actually happens is the compiler invokes an _implicit conversion_ from an instance of the type to a wrapper type that has the desired method. \n\nFor example, suppose I want to add a `toJSON` method to my `Person` type above, but I don't want this added to the class itself. Maybe it's from a library that I can't modify. Maybe I only want this method in some contexts, but I don't want its baggage everywhere. Here's how to do it.\n\n```scala\n// recall: case class Person(firstName: String, lastName: String, age: Int)\nimplicit class PersonToJSON(person: Person) {\n  // Just return a JSON-formatted string, for simplicity of the example:\n  def toJSON: String = \n    s\"\"\"{ \"firstName\": ${person.firstName}, \"lastName\": ${person.lastName}, \"age\": ${person.age} }\"\"\"\n}\n...\nval p = Person(\"Dean\", \"Wampler\", 39)\np.toJSON   // Like magic!!\n```\n\nThe `implicit` keyword tells the compiler to consider `PersonToJSON` when I attempt to call `toJSON` on a `Person` instance. The compiler finds this implicit class and does the conversion implicitly, then calls the `toJSON` method.\n\nThere are many other uses for implicits. They are a powerful implementation tool for various design problems, but they have to be used wisely, because it can be difficult for the reader to know what's going on."
  }, {
    "metadata" : {
      "id" : "43D51BE94F464AE8B794F673544EF5D3"
    },
    "cell_type" : "markdown",
    "source" : "## 9H. Sealed Type Hierarchies\nAn important concept in modern languages is _sum types_, where there is a finite set of possible instances. Two examples from Scala are `Option[T]` and its allowed subtypes `Some[T]` and `None`, and `Either[L,R]` and its subtypes `Left[L]` and `Right`[R]`.\n\nNote that `Option[T]` represents two and only two possible states, either I have something, a `T` inside a `Some[T]`, or I don't anything, a `None`. There are no additional \"states\" that are logically possible for the `Option[T]` \"abstraction\". Similarly, `Either[L,R]` encapsulates a similar dichotomy, often used for \"failure\" (e.g., `Left[Throwable]` by convention) and \"successful result\" (`Right[T]` for some \"expected\" `T`).\n\nThe term _sum type_ comes from an analog between types and arithmetic. For `Option`, the number of allowed intances (ignoring the type parameter `T`) is just the sum, _two_. Similarly for `Either`.\n\nThere are also _product types_, like tuples, where you can combining types together _multiples_ the number of instances. For example, a tuple of `(Option,Either)` would have 2*2 instances. A tuple `(Boolean,Option,HTTP_Commands)` has 2*2*7 possible instances (there are 7 HTTP 1.1 commands, like `GET`, `POST`, etc.)\n\nScala use type hierarchies for sum types, where an abstract _sealed_ trait or class is used for the base type, e.g., `Option[T]` and `Either[L,R]`, and subtypes represent the concrete types. The `sealed` keyword is used on the base type and it is crucial; it tells the compiler to only allow subtypes to be defined in the same _file_, which means users can't add their own subtypes, breaking the logic of the type.\n\nSome other languages implement sumtypes using a variation of _enumerations_. Jave has that, but it's a much more limited concept than true subtypes."
  }, {
    "metadata" : {
      "id" : "EEA7210546F049D58D92A8C97F4A69E7"
    },
    "cell_type" : "markdown",
    "source" : "## 9I. Option Type Broken in Java\nSpeaking of `Option[T]`, Java 8 introduced a similar type called `Optional`. (The name `Option` was already used for something else.) However, it's design has some subtleties that make the behavior not straightforward when `nulls` are involved. For details, see [this blog post](https://developer.atlassian.com/blog/2015/08/optional-broken/)."
  }, {
    "metadata" : {
      "id" : "7C1D0E4BCDD24C9287BF14D74FDFE97D"
    },
    "cell_type" : "markdown",
    "source" : "## 9J: Definition-site Variance vs. Call-site Variance\nThis is a technical point. In Java, when you define a collection, say `Bucket[T]` to hold items of some type `T`, you can't specify in the declaration whether it's okay to substitute a subtype of `Bucket` with a subtype of `T`. For example, is the following okay?:\n```java\n// Java\nBucket<Object> bucket = new MyBucketSubtype<String>()\n```\nThis substitutability is called _variance_, referring to the variance allowed in `T`. Java forces you to specify the variance at the _call site_:\n```java\nBucket<? extends Object> bucket = new MyBucketSubtype<String>()\n```\nThis is harder for the user. It's much better if the implementer of `Bucket[T]`, who understands the desired behavior, defines the allowed variance behavior at the _definition site_:\n```scala\n// Scala\n// declaration:\nclass Bucket[+T] {  // The + means the subclassing we're discussing is allowed.\n...\n}\n// usage:\nval bucket: Bucket[AnyRef] = new MyBucketSubtype[String]()\n```"
  }, {
    "metadata" : {
      "id" : "069B350306444D689D66BE85E7C96B21"
    },
    "cell_type" : "markdown",
    "source" : "## 9K: Value Classes\nScala's built-in _value types_ `Int`, `Long`, `Float`, `Double`, `Boolean`, and `Unit` are implemented with the corresponding JVM primitive values, eliminating the overhead of allocating an instance on the heap. What if you define a class that wraps _one_ of these values?\n```scala\nclass Celsius(value: Float) {\n  // methods\n}\n```\nUnfortunately, instances are allocated on the heap, even though all instance \"state\" is held by a single primitive `Float`. Scala now has an `AnyVal` trait. If you extend it with classes like `Celsius`, they will enjoy the same optimization that the built-in value types enjoy.\n```scala\nclass Celsius(value: Float) extends AnyVal {\n  // methods\n}\n```\n\nSo, why doesn't Scala make this optimization automatically? There are some limitations, which are described [here](http://docs.scala-lang.org/overviews/core/value-classes.html) and in [my book](http://shop.oreilly.com/product/0636920033073.do)."
  }, {
    "metadata" : {
      "id" : "BF296220FD9B4E1987610CDFE7D86C91"
    },
    "cell_type" : "markdown",
    "source" : "## 9L. Lazy Vals\nSometimes you don't want to initialize a value if you don't need. For example, a database connection is expensive.\n```scala\nlazy val jdbcConnection = new JDBCConnection(...)\n```\nPrefix with the `lazy` keyword to delay initialization until it's actually needed (if ever). This feature can also be used to solve some tricky \"order of initialization\" problems."
  }, {
    "metadata" : {
      "id" : "E1AC9A4F004249AAAD14813AF7AAC1BC"
    },
    "cell_type" : "markdown",
    "source" : "What about the other languages? \n* **Python:** Offers equivalents for some these features.\n* **R:** Supports some of these features.\n* **Java:** Supports none of these features."
  }, {
    "metadata" : {
      "id" : "455487BFC1784B8292289A40B70BFAA5"
    },
    "cell_type" : "markdown",
    "source" : "# But Scala Has Some Disadvantages...\n\nAll of the advantages discussed above make Scala code quite concise, especially compared to Java code.\n\nHowever, no language is perfect. You should know about the disadvantages, too."
  }, {
    "metadata" : {
      "id" : "D8DD3106FCBF435FAC27E3D75850AA16"
    },
    "cell_type" : "markdown",
    "source" : "## 1. Data-centric Tools and Libraries\n\nThe R and Python communities have a much wider selection of data-centric tools and libraries, including better integrations with rich charting libraries, like [ggplot2](http://ggplot2.org/) for R."
  }, {
    "metadata" : {
      "id" : "E23284F1547540108DEC21B524BF8F3C"
    },
    "cell_type" : "markdown",
    "source" : "## 2. The JVM Has Some Issues "
  }, {
    "metadata" : {
      "id" : "AA07596741F84C4DBDBD223B5D47B02C"
    },
    "cell_type" : "markdown",
    "source" : "### Integer indexing of arrays\n\nBecause Java has _signed_ integers, this effectively limits array sizes to 2 Billion. Therefore, _byte_ arrays, which are often used for holding data, are limited to 2GB. This is in an era when _terabyte_ heaps (TB) are possible!"
  }, {
    "metadata" : {
      "id" : "66A59310099B46E985F8B0761076E1E3"
    },
    "cell_type" : "markdown",
    "source" : "### Inefficiency of the JVM Memory Model\nThe JVM has a very flexible, general-purpose model of organizing data into memory and managing garbage collection. However, for massive data sets of records with the same (or nearly the same schema), it is extremely inefficient. Spark's [Tungsten Project](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html) is addressing this problem by introducing custom object-layouts, managed memory, and code generation for other performance bottlenecks. "
  }, {
    "metadata" : {
      "id" : "75724F7135BA48E985A5D39CB00CEB42"
    },
    "cell_type" : "markdown",
    "source" : "Here is how Java typically lays out objects in memory. Note the references to small, discontiguous chunks of memory, each of which eventually becomes garbage to manage. Also, there is poor CPU cache efficiency here."
  }, {
    "metadata" : {
      "id" : "64B3B23B30AF4BE78CF40AC489846064"
    },
    "cell_type" : "markdown",
    "source" : "<img src='https://raw.githubusercontent.com/data-fellas/scala-for-data-science/master/notebooks/images/JavaMemory.jpg' alt='Typical Java Object Layout' height='414' width='818'></img>"
  }, {
    "metadata" : {
      "id" : "283CC816BE0B4CA280FC20E2D2A7F412"
    },
    "cell_type" : "markdown",
    "source" : "Instead, Tungsten uses a more efficient, cache-friendly layout like this:"
  }, {
    "metadata" : {
      "id" : "E42E5F36D8714FDBA039F6179D45DFA8"
    },
    "cell_type" : "markdown",
    "source" : "<img src='https://raw.githubusercontent.com/data-fellas/scala-for-data-science/master/notebooks/images/TungstenMemory.jpg' alt='Tungsten Object Layout' height='264' width='818'></img>"
  }, {
    "metadata" : {
      "id" : "9EF11FF9CCDA43CE838105419BACAA0C"
    },
    "cell_type" : "markdown",
    "source" : "## Scala REPL Wierdness\n\nThe way the Scala REPL (interpreter) compiles code leads to memory leaks, which cause problems when working with big data sets and long sessions. There are some other issues, too. \n\nSee Dean's talk at [Strata San Jose](http://conferences.oreilly.com/strata/hadoop-big-data-ca/public/schedule/detail/47105) ([extended slides](http://deanwampler.github.io/polyglotprogramming/papers/ScalaJVMBigData-SparkLessons-extended.pdf)) for more details. This talk also discusses the Tungsten optimizations in Spark."
  } ],
  "nbformat" : 4
}